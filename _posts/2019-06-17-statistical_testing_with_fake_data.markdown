---
layout: post
title:      "Statistical Testing with Fake Data"
date:       2019-06-17 12:56:41 +0000
permalink:  statistical_testing_with_fake_data
---


Original copy posted on Medium: https://medium.com/@jericksen20/statistical-analysis-with-fake-data-1c55857f0726?postPublishedType=repub

Statistical Testing with Fake Data

A brief walkthrough of a real process used for testing fake data.

A recent statistical analysis project took me on a journey through some fake data to uncover some real relationships. I’m new to statistical analysis, and data science for that matter, but the analysis and practices used to test for statistical differences were real. Here, I walk through the techniques, python libraries, processes, and tests used to accept or reject our null hypothesis’.

The Data
Once upon a time, Microsoft decided to create a fake database that housed data generated by a fictitious company: Northwind. It’s a relational database with tables containing data on orders filled (shipped), vendors, employees, revenue, products, et cetera. The idea for this project was to conduct statistical tests to assess whether or not the questions posed by ‘stakeholders’ could be answered using the scientific process.

The Process
The begin, we had to connect with the database for running our queries using a Jupyter Notebook. For this, I chose to work with SQLAlchemy — a handy package with some neat functionality. After importing the library, I connected with the database and ran the inspect method to assess the tables available:

import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy.orm import Session, sessionmaker
from sqlalchemy import inspect
engine = create_engine("sqlite:///Northwind_small.sqlite", echo=True)
Session = sessionmaker(bind=engine)
session = Session()
inspector = inspect(engine)
table_names = inspector.get_table_names()
table_names
# Tables Returned:
['Category',
 'Customer',
 'CustomerCustomerDemo',
 'CustomerDemographic',
 'Employee',
 'EmployeeTerritory',
 'Order',
 'OrderDetail',
 'Product',
 'Region',
 'Shipper',
 'Supplier',
 'Territory']
SQLAlchemy allows a SQL query to run while writing the returned data to a pandas data frame. I used this frequently throughout the project as makes EDA and statistical testing much easier. Below is an example query I used which built a data frame for easy analysis and testing:

product_descriptions_df = pd.read_sql_query(
    '''
    SELECT
    
    p.productname,
    c.categoryname, 
    c.description
    FROM product AS p
    LEFT JOIN category AS c
    ON p.categoryid = c.id
''', engine)
product_descriptions_df.head(10)
The resulting output:


Once I had my workflow established for retrieving data from the DB file, it was time for running statistical tests to answer basic questions about the data.

Statistical tests can be useful tools for challenging previous, non-data established assumptions about the world around us. Statistical tests are used frequently during the scientific process as a way to challenge or accept previously held beliefs or knowledge about the natural world. By extension, we can apply these same statistical tests to challenge or accept conventions held by employees or management within a company.

The scientific process, as much of us have learned in grade school, involves establishing a ‘null hypothesis’ and an ‘alternative hypothesis’. In simple terms, the null hypothesis states what is already known or has been assumed, while the alternative hypothesis seeks to challenge whether or not the null hypothesis can be accepted knowledge (or convention).

To conduct these experiments using the Northwind data, we first asked a chain of questions centered around the business operations followed by establishing our null and alternative hypothesis for each question. Following that, it was time to run our tests to either accept or reject our null hypothesis.

An example below:

Do discounts have a statistically significant effect on the number of products customers order? If so, at what level(s) of discount?
The question has two parts. Part one can be reconfigured to the following null and alternative hypothesis:

Ho: 𝜇1=𝜇2 - Control group = Treatment group, i.e., the mean number of products sold with and without a discount is equal.
Ha: 𝜇1≠𝜇2 - Control group ≠ Treatment group, i.e., the mean number of products sold with a discount is statistically larger, or smaller than the mean number of products sold with a discount.
Part two of the above question can be reconfigured as the following:

Ho: 𝜇1=𝜇2=…𝜇𝑘 — Control group = Treatment group at all levels of discount, i.e, the number of products sold with differing levels of discounts do not vary from products sold without a discount.
Ha: 𝜇1≠𝜇2≠…𝜇𝑘 — Control group ≠ Treatment group at all levels of discount, i.e, the number of products sold with differing levels of discounts do vary from products sold without a discount.
The tests needed to asses a hypothesis come in a number of forms. A discussion of the various tests is outside the scope of this post. For the questions above, I chose to run a two-sample t-test for part one, and a tukey test for part two.

The process for running these tests require an assessment of the distributions for both the test and control groups. The built-in assumptions with these tests require normality of distribution and an equivalent variance between the distributions. The following is a summation of the steps used to answer part one.

Let’s walk through the answer to part 1:

Isolate the control and treatment groups using a separate data frame for each
# Orders with no discounts:
orders_no_discount = pd.read_sql_query(
    '''
    SELECT quantity
    FROM orderdetail
    WHERE discount = 0
    ''', engine)
orders_no_discount.head()
# Orders with a discount:
orders_w_discount = pd.read_sql_query(
    '''
    SELECT quantity
    FROM orderdetail
    WHERE discount > 0
    ''', engine)
orders_w_discount.head()
2. Test for normality within the distributions using the shapiro test:

no_disc_norm = stats.shapiro(orders_no_discount)
w_disc_norm = stats.shapiro(orders_w_discount)
print('The p_value for orders with no discounts is:', no_disc_norm[1])
print('The p_value for order with a discount is:', w_disc_norm[1])
# Output:
The p_value for orders with no discounts is: 3.803856556577728e-34
The p_value for order with a discount is: 6.88120409395894e-26
3. With low p-values, we accept the distributions are non-normal and apply the Central Limit Theorem by taking the mean of 100 separate samples drawn from the parent population. Sample sizes were 30 each:

no_discount_samps = [orders_no_discount['Quantity'].sample(n=30, replace = True).mean() for i in range (100)]
w_discount_samps = [orders_w_discount['Quantity'].sample(n=30, replace = True).mean() for i in range (100)]
4. Plot the distributions using seaborn for visual reference:

plt.hist(no_discount_samps, label = 'Order Size No Discount', alpha = .5, color = 'r', bins=20)
plt.hist(w_discount_samps, label = 'Order Size With Discount', alpha = .5, bins=20)
plt.title('Mean Order Size', size = 18)
plt.xlabel('Products per Order', size = 14)
plt.ylabel('Frequency', size = 14)
plt.legend()
plt.show()

5. Run the t-test using scipy stats:

tstat, pval = scs.ttest_ind(w_discount_samps, no_discount_samps)
print('T-Value = ', tstat)
print('P-Value = ', pval)
# The output:
T-Value =  11.199520190145822
P-Value =  7.195970232740309e-23
6. Interpret the results: with a p-value below our established alpha value of .05, we can reject the null hypothesis that discounts do not affect order size.

Enlighting stuff! Not really. But the point here was to illustrate the workflow I established when conducting some basic statistical tests using fake data. For this project, a number of tests were run using the same workflow and tools. For anyone looking to conduct similar tests with a similar database, I highly recommend SQLAlchemy with its ability to write the data returned from your queries directly into a data frame.

Further, it’s important to understand the built-in assumptions for the various statistical tests. In the example above, the assumption of normality within the data must be met in order to trust our results. Because the data had a non-normal distribution, we were required to solve for this by applying the Central Limit Theorem via a re-sampling method to establish our normal distributions for reliable testing.

Hope you found this helpful!


